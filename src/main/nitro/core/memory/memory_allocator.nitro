
// Block Header has 8 bytes
let MEMORY_ALLOCATOR_BLOCK_HEADER_SIZE: Int = 8
// By default 7 entries are used to get a total block size of 64 (usual cache line size)
let MEMORY_ALLOCATOR_ENTRIES_PER_BLOCK: Int = 7
// MEMORY_ALLOCATOR_BLOCK_HEADER_SIZE + MEMORY_ALLOCATOR_ENTRIES_PER_BLOCK * size_of<MemoryAllocatorBlockEntry>()
let MEMORY_ALLOCATOR_BLOCK_SIZE: Int = 64

let MASK_USED_ENTRY: Int = 0x80000000
let MASK_SIZE_ENTRY: Int = 0x7FFFFFFF

let TOTAL_AVAILABLE_ENTRIES_THRESHOLD: Int = MEMORY_ALLOCATOR_ENTRIES_PER_BLOCK + 4
let MEMORY_ALLOCATOR_MARK_FREE: Boolean = false

// Global allocator instance
@InitPriority
let global_allocator: MemoryAllocator = MemoryAllocator::new(get_memory(), 1048576 /* 1 MB */)

// General purpose memory allocator
// Allows alloc/free with arbitrary sizes
@NoGC
struct MemoryAllocator {
    arena: MemoryArena
    // True if the allocator is correctly initialized
    initialized: Boolean
    // Min amount to grow when no memory is available
    min_grow_size: Int
    // Stats to defect memory leaks
    user_alloc_total: Int
    user_free_total: Int
    // Allows to known when to allocate a new blocks
    total_available_entries: Int
    // Fallback to arena allocator when some internal operation requires memory allocation
    enable: Boolean
    // Linked list of blocks
    block_list: Ptr<MemoryAllocatorBlock>
}

// Each block holds several entries, each correspond to an allocation that can be in use or free
// Size MEMORY_ALLOCATOR_BLOCK_SIZE bytes (header + MEMORY_ALLOCATOR_ENTRIES_PER_BLOCK entries)
@NoGC
struct MemoryAllocatorBlock {
    free_size: Int
    next: Ptr<MemoryAllocatorBlock>
    // Has MEMORY_ALLOCATOR_ENTRIES_PER_BLOCK consecutive MemoryAllocatorBlockEntry entries
}

// Encode the block pointer and the entry index in a single 64 bit integer, to avoid allocation Pair<Block, Int>
type_alias MemoryAllocatorBlockEntryId = Long

// Size 8 bytes
@NoGC
struct MemoryAllocatorBlockEntry {
    // bit 31 is flag 'used'
    // rest is size
    flags: Int
    // Null pointer indicates unused entry that can be overwritten
    data: Ptr<Byte>
}

// Initialize a new memory allocator
fun MemoryAllocator::new(arena: MemoryArena, min_grow_size: Int): MemoryAllocator {
    // Header
    let header: MemoryAllocator = arena.alloc<MemoryAllocator>().unsafe_as_ref()
    header.arena = arena
    header.initialized = false
    header.min_grow_size = min_grow_size
    header.user_alloc_total = 0
    header.user_free_total = 0
    header.total_available_entries = MEMORY_ALLOCATOR_ENTRIES_PER_BLOCK
    header.enable = true
    header.block_list = null_ptr()

    let first_block: MemoryAllocatorBlock = arena.alloc_bytes(MEMORY_ALLOCATOR_BLOCK_SIZE, MEMORY_ALLOCATOR_BLOCK_SIZE).unsafe_cast_as_ref()
    header.init_new_block(first_block)

    let entry: MemoryAllocatorBlockEntry = first_block[0]
    entry.flags = MASK_USED_ENTRY | MEMORY_ALLOCATOR_BLOCK_SIZE
    entry.data = ptr_of(first_block).unsafe_cast()

    header.initialized = true
    header.grow_memory(min_grow_size)
    header.ensure_enough_entries()

    ret header
}

// Allocate with the size of the given type
fun MemoryAllocator.alloc<#Value>(): Ptr<#Value> {
    ret this.alloc_bytes(size_of<#Value>).unsafe_cast()
}

// Allocate with the size given and zero out the memory
fun MemoryAllocator.zero_alloc_bytes(requested_size: Int): Ptr<Byte> {
    let ptr = alloc_bytes(requested_size)
    memory_fill_internal(ptr.address, 0.to_byte(), requested_size)
    ret ptr
}

fun MemoryAllocator.alloc_bytes(requested_size: Int): Ptr<Byte> {
    ret alloc_bytes(requested_size, 4)
}

// Allocate memory with at least the requested size, must be freed with free()
// The result will NEVER be a null pointer, even if the requested size is 0, if no memory is available, a crash will occur
// The alignment must be a power of two, 1, 2, 4, 8, etc. (otherwise it will be rounded up to the next power of two)
fun MemoryAllocator.alloc_bytes(requested_size: Int, alignment: Int): Ptr<Byte> {
    if !initialized {
        enable = false
        crash_with_info("MemoryAllocator being used before initialization")
    }

    // Closest power of two greater than the alignment
    alignment = alignment.round_up_to_power_of_two()

    // Min allocation size is 4 bytes, even if you ask for 0 bytes, some memory is allocated to return an unique pointer every time
    let user_alloc_size = max(Ptr::align(requested_size), 4)

    // Fallback for internal operations that require memory allocation
    if !enable {
        ret arena.alloc_bytes(user_alloc_size, alignment)
    }

    ensure_enough_entries()

    let entry_id = find_free_entry_for(user_alloc_size, alignment)

    if entry_id == 0L {
        // No enough free space, request more memory,
        // at least `min_grow_size`, unless the user requested more than that
        grow_memory(max(min_grow_size, user_alloc_size))

        entry_id = find_free_entry_for(user_alloc_size, alignment)

        if entry_id == 0L {
            enable = false
            crash_with_info("Out of memory")
        }
    }

    let entry: MemoryAllocatorBlockEntry = MemoryAllocatorBlockEntry::get_entry_by_id(entry_id)
    let entry_block = MemoryAllocatorBlockEntry::get_entry_block_by_id(entry_id)
    let entry_size = entry.flags & MASK_SIZE_ENTRY

    let misaligned_size = (entry.data.address & (alignment - 1))
    if misaligned_size != 0 {
        // This `entry` will hold the unaligned part
        // aligned part will be assigned to a new entry
        let unaligned_size = alignment - misaligned_size
        let aligned_size = entry_size - unaligned_size

        if aligned_size == 0 {
            enable = false
            // find_free_entry_for() should never return an entry with less than unaligned_size + 4 bytes
            crash_with_info("Internal error: aligned_size is 0")
        }

        // Get an available entry to store the remaining memory
        let new_entry_id = alloc_entry()
        let new_entry: MemoryAllocatorBlockEntry = MemoryAllocatorBlockEntry::get_entry_by_id(new_entry_id)
        let new_entry_block: MemoryAllocatorBlock = MemoryAllocatorBlockEntry::get_entry_block_by_id(new_entry_id)
        let new_entry_index: Int = MemoryAllocatorBlockEntry::get_entry_index_by_id(new_entry_id)

        // Update this entry with the unaligned part
        entry.flags = unaligned_size
        // entry.data is already pointing to the correct location

        entry_block.free_size -= entry_size
        entry_block.free_size += unaligned_size

        // Update other entry with the aligned part
        new_entry.flags = aligned_size
        new_entry.data = entry.data.offset_in_bytes(unaligned_size)
        new_entry_block.free_size += aligned_size

        // Change the selected entry to the aligned one
        entry = new_entry
        entry_block = new_entry_block
        entry_size = aligned_size
    }

    // Mark the entry as used to prevent further internal allocations from using it
    entry.flags = entry.flags | MASK_USED_ENTRY
    entry_block.free_size -= entry_size

    // Split the entry if is too large for the requested size
    if entry_size > user_alloc_size * 2 {
        let new_entry_id = alloc_entry()
        let new_entry: MemoryAllocatorBlockEntry = MemoryAllocatorBlockEntry::get_entry_by_id(new_entry_id)
        let new_entry_block: MemoryAllocatorBlock = MemoryAllocatorBlockEntry::get_entry_block_by_id(new_entry_id)
        let new_entry_index: Int = MemoryAllocatorBlockEntry::get_entry_index_by_id(new_entry_id)

        let new_entry_size = entry_size - user_alloc_size
        let old_entry_size = user_alloc_size

        // Mark entry as used and update its size
        entry.flags = MASK_USED_ENTRY | old_entry_size

        // Not used, size is the remaining size
        new_entry.flags = new_entry_size

        // Points to the remaining memory
        new_entry.data = entry.data.offset_in_bytes(old_entry_size)

        // Add the new entry as free space to its block
        new_entry_block.free_size += new_entry_size
    }

    user_alloc_total += entry.flags & MASK_SIZE_ENTRY

    ret entry.data
}

fun MemoryAllocator.alloc_entry(): MemoryAllocatorBlockEntryId {
    total_available_entries -= 1
    let new_entry_id = find_available_entry()

    if new_entry_id == 0L {
        enable = false
        crash_with_info("No available entries")
    }

    ret new_entry_id
}

// Try to extend the allocation, if possible, otherwise alloc new memory and copy
fun MemoryAllocator.realloc_bytes(ptr: Ptr<Byte>, new_size: Int): Ptr<Byte> {
    if !initialized {
        enable = false
        crash_with_info("MemoryAllocator being used before initialization")
    }
    let entry_id = find_entry_by_data_ptr(ptr)

    if entry_id == 0L {
        enable = false
        crash_with_info("Invalid pointer: $ptr")
    }

    let entry: MemoryAllocatorBlockEntry = MemoryAllocatorBlockEntry::get_entry_by_id(entry_id)
    let block: MemoryAllocatorBlock = MemoryAllocatorBlockEntry::get_entry_block_by_id(entry_id)
    let index = MemoryAllocatorBlockEntry::get_entry_index_by_id(entry_id)
    let entry_used = entry.flags & MASK_USED_ENTRY != 0
    let entry_size = entry.flags & MASK_SIZE_ENTRY

    if !entry_used {
        enable = false
        crash_with_info("Calling realloc with freed pointer: $ptr")
    }

    if entry_size >= new_size {
        // No need to change the size
        ret ptr
    }

    let new_data_ptr = ptr_from_address<Byte>(entry.data.address + entry_size)
    let contiguous_entry_id = find_entry_by_data_ptr(new_data_ptr)

    // No contiguous entry, fallback to alloc new memory and copy
    if contiguous_entry_id == 0L {
        let new_ptr = alloc_bytes(new_size)
        memory_copy_internal(new_ptr.address, ptr.address, entry_size);
        free(ptr)
        ret new_ptr
    }

    let contiguous_entry: MemoryAllocatorBlockEntry = MemoryAllocatorBlockEntry::get_entry_by_id(contiguous_entry_id)
    let contiguous_entry_block: MemoryAllocatorBlock = MemoryAllocatorBlockEntry::get_entry_block_by_id(contiguous_entry_id)
    let contiguous_entry_used = contiguous_entry.flags & MASK_USED_ENTRY != 0
    let contiguous_entry_size = contiguous_entry.flags & MASK_SIZE_ENTRY

    let total_size = entry_size + contiguous_entry_size

    // Contiguous entry is used or doesn't have enough space, fallback to alloc new memory and copy
    if contiguous_entry_used || total_size < new_size {
        let new_ptr = alloc_bytes(new_size)
        memory_copy_internal(new_ptr.address, ptr.address, entry_size);
        free(ptr)
        ret new_ptr
    }

    let remaining_size = total_size - new_size

    if remaining_size > 16 {
        // Balance entries, move space from the second entry to the first
        entry.flags = MASK_USED_ENTRY | new_size

        // Note: free_size is not updated because `entry` is not free, so the amount of free space is the same

        // Update flags with the remaining size and without MASK_USED_ENTRY
        contiguous_entry.flags = remaining_size
        contiguous_entry.data = ptr.offset_in_bytes(new_size)

        // Decrease free size to account lost space
        contiguous_entry_block.free_size -= new_size - entry_size
    } else {
        // Merge 2 entries into one
        entry.flags = MASK_USED_ENTRY | total_size

        // Mark available
        contiguous_entry.flags = 0
        contiguous_entry.data = null_ptr()
        contiguous_entry_block.free_size -= contiguous_entry_size
        total_available_entries += 1
    }
    ret ptr
}

// Free memory previously allocated
// This function will crash if the pointer is invalid or if it was already freed before
fun MemoryAllocator.free(ptr: Ptr<Byte>) {
    if !initialized {
        enable = false
        crash_with_info("MemoryAllocator being used before initialization")
    }

    // Maybe this could be optimized storing a pointer just before the data, but that will use more memory
    let entry_id = find_entry_by_data_ptr(ptr)

    if entry_id == 0L {
        enable = false
        crash_with_info("Invalid pointer: $ptr")
    }

    let entry: MemoryAllocatorBlockEntry = MemoryAllocatorBlockEntry::get_entry_by_id(entry_id)
    let block: MemoryAllocatorBlock = MemoryAllocatorBlockEntry::get_entry_block_by_id(entry_id)
    let entry_size = entry.flags & MASK_SIZE_ENTRY
    let entry_used = entry.flags & MASK_USED_ENTRY != 0

    if !entry_used {
        enable = false
        crash_with_info("Double free: $ptr")
    }

    // Mark the memory to detect usage after free
    if MEMORY_ALLOCATOR_MARK_FREE {
        memory_fill_internal(ptr.address, 0xAB.to_byte(), entry_size)
    }

    // Update the entry
    entry.flags = entry_size
    block.free_size += entry_size
    user_free_total += entry_size
}

// Increase the memory available to the allocator
fun MemoryAllocator.grow_memory(size: Int) {
    let region: Ptr<Byte> = arena.alloc_bytes(Ptr::align(size), 64)

    let entry_id = find_available_entry()

    if entry_id == 0L {
        enable = false
        crash_with_info("Unable to extend memory, no available entries ($total_available_entries)")
    }

    let entry = MemoryAllocatorBlockEntry::get_entry_by_id(entry_id)
    let block = MemoryAllocatorBlockEntry::get_entry_block_by_id(entry_id)

    // Store 0 as pointer to the next free chunk
    let int_ptr: IntPtr = region.unsafe_cast_as_ref()
    int_ptr.value = 0

    // Initialize entry and block
    entry.flags = size & MASK_SIZE_ENTRY
    entry.data = region
    block.free_size += size

    // Create new entries
    total_available_entries -= 1
    ensure_enough_entries()
}

// Allocate a memory arena, a chunk of memory that allows contiguous allocations
fun MemoryAllocator.alloc_memory_arena(size: Int): MemoryArena {
    let new_arena: MemoryArena = alloc_bytes(size_of<MemoryArena>() + Ptr::align(size)).unsafe_cast_as_ref()
    new_arena.len = 0
    // Bytes is an Array<Byte> with capacity and elements contiguous in memory
    // We overlap the start of the array with the end of the MemoryArena struct,
    // so the capacity of the array and arena are in the same memory location
    new_arena.bytes = ptr_of(new_arena).offset_in_bytes(size_of<MemoryArena>() - 4).unsafe_cast_as_ref()
    new_arena.capacity = Ptr::align(size)
    ret new_arena
}

// Free an arena previously allocated with alloc_memory_arena()
fun MemoryAllocator.free_memory_arena(memory_arena: MemoryArena) {
    free(ptr_of(memory_arena).unsafe_cast())
}

fun MemoryAllocator.ensure_enough_entries() {
    // Keep at least 4 entries available, needed to grow the memory and to allocate new blocks
    while total_available_entries < TOTAL_AVAILABLE_ENTRIES_THRESHOLD {
        create_new_block()
    }
}

// Find an entry that can be overwritten to point to new data
fun MemoryAllocator.find_available_entry(): MemoryAllocatorBlockEntryId {
    let block_ptr: Ptr<MemoryAllocatorBlock> = block_list

    loop {
        if block_ptr.is_null() {
            ret 0L
        }

        let block: MemoryAllocatorBlock = block_ptr.unsafe_as_ref()

        // Find a free entry
        repeat MEMORY_ALLOCATOR_ENTRIES_PER_BLOCK {
            if block[it].data.is_null() {
                ret MemoryAllocatorBlockEntry::to_id(block, it)
            }
        }

        // Entry not found, next
        block_ptr = block.next
    }

    ret 0L
}

// Find a free entry with enough space
fun MemoryAllocator.find_free_entry_for(user_alloc_size: Int, alignment: Int): MemoryAllocatorBlockEntryId {
    // Find a block with enough space
    let block_ptr: Ptr<MemoryAllocatorBlock> = block_list

    loop {
        if block_ptr.is_null() {
            break
        }

        let block: MemoryAllocatorBlock = block_ptr.unsafe_as_ref()

        // Not enough space, next
        if block.free_size < user_alloc_size {
            block_ptr = block.next
            continue
        }

        // Find a free entry
        repeat MEMORY_ALLOCATOR_ENTRIES_PER_BLOCK {
            let entry: MemoryAllocatorBlockEntry = block[it]

            // Invalid entry
            if entry.data.is_null() {
                continue
            }

            let used = entry.flags & MASK_USED_ENTRY != 0
            let size = entry.flags & MASK_SIZE_ENTRY

            // Entry in use
            if used {
                continue
            }
            // Not enough space
            if size < user_alloc_size {
                continue
            }

            // Not aligned and cannot be split into aligned and unaligned parts
            let misaligned_size = entry.data.address & (alignment - 1)
            if misaligned_size != 0 && size < user_alloc_size + (alignment - misaligned_size) {
                continue
            }

            // Entry found
            ret MemoryAllocatorBlockEntry::to_id(block, it)
        }

        // Entry not found, next
        block_ptr = block.next
    }

    ret 0L
}

// Find an entry by the data pointer
fun MemoryAllocator.find_entry_by_data_ptr(data: Ptr<Byte>): MemoryAllocatorBlockEntryId {
    // Find a block with enough space
    let block_ptr: Ptr<MemoryAllocatorBlock> = block_list
    let index = 0

    loop {
        if block_ptr.is_null() {
            break
        }

        let block: MemoryAllocatorBlock = block_ptr.unsafe_as_ref()

        repeat MEMORY_ALLOCATOR_ENTRIES_PER_BLOCK {
            if block[it].data.address == data.address {
                ret MemoryAllocatorBlockEntry::to_id(block, it)
            }
        }

        block_ptr = block.next
        index += 1
    }

    ret 0L
}

fun MemoryAllocator.create_new_block(): MemoryAllocatorBlock {
    // Prevent recursion
    total_available_entries += MEMORY_ALLOCATOR_ENTRIES_PER_BLOCK

    // Alloc and init block
    let block: MemoryAllocatorBlock = alloc_bytes(MEMORY_ALLOCATOR_BLOCK_SIZE, MEMORY_ALLOCATOR_BLOCK_SIZE).unsafe_cast_as_ref()
    init_new_block(block)

    ret block
}

// Internally allocate a new block and append it to the block list
fun MemoryAllocator.init_new_block(block: MemoryAllocatorBlock) {
    block.free_size = 0
    block.next = null_ptr()

    // Zero out entries
    repeat MEMORY_ALLOCATOR_ENTRIES_PER_BLOCK {
        let entry: MemoryAllocatorBlockEntry = block[it]
        entry.flags = 0
        entry.data = null_ptr()
    }

    // First block
    if block_list.is_null() {
        block_list = ptr_of(block)
        ret
    }

    // Append to the last block
    let curr_block: MemoryAllocatorBlock = block_list.unsafe_as_ref()

    loop {
        if curr_block.next.is_null() {
            curr_block.next = ptr_of(block)
            break
        }
        curr_block = curr_block.next.unsafe_as_ref()
    }
}

fun MemoryAllocator.detect_corruption() {
    let save_user_alloc_total = this.user_alloc_total
    let save_user_free_total = this.user_free_total
    let save_total_available_entries = this.total_available_entries

    let total_free_bytes = 0
    let total_used_bytes = 0

    let block_ptr: Ptr<MemoryAllocatorBlock> = block_list

    while !block_ptr.is_null() {
        let block: MemoryAllocatorBlock = block_ptr.unsafe_as_ref()
        let count_free_size = 0
        let count_used_size = 0

        repeat MEMORY_ALLOCATOR_ENTRIES_PER_BLOCK {
            let entry: MemoryAllocatorBlockEntry = block[it]
            let used = entry.flags & MASK_USED_ENTRY != 0
            let size = entry.flags & MASK_SIZE_ENTRY

            if used {
                count_used_size += size
            } else {
                count_free_size += size
            }

            if used {
                if entry.data.is_null() {
                    enable = false
                    println("Corrupted entry: ${ptr_of(entry)}")
                    crash_with_info("Memory corruption detected")
                }

                if !is_block_ptr(entry.data) {
                    debug_detect_overlap(entry.data, size)
                }
            }
        }

        if count_free_size != block.free_size {
            enable = false
            println("Block corruption at ${ptr_of(block)}: block.free_size = ${block.free_size}, real free size = $count_free_size")
            println("MemoryAllocator: total_free_bytes = $total_free_bytes, total_used_bytes = $total_used_bytes")
            crash_with_info("Memory corruption detected")
        }

        total_free_bytes += count_free_size
        total_used_bytes += count_used_size

        block_ptr = block.next
    }
}

fun MemoryAllocator.is_block_ptr(ptr: Ptr<Byte>): Boolean {
    let block_ptr: Ptr<MemoryAllocatorBlock> = block_list

    while !block_ptr.is_null() {
        if MemoryAllocator::is_bad_address(block_ptr.unsafe_cast()) {
            crash("Internal allocator data corrupted")
        }

        let block: MemoryAllocatorBlock = block_ptr.unsafe_as_ref()

        if block_ptr.address == ptr.address {
            ret true
        }

        block_ptr = block.next
    }
    ret false
}

fun MemoryAllocator.crash_with_info(msg: String) {
    enable = false
    debug()
    crash(msg)
}

fun MemoryAllocator::is_bad_address(ptr: Ptr<Byte>): Boolean {
    ret ptr.address == 0xCDCDCDCD || ptr.address == 0xABACABAB || ptr.address == 0xFFFFFFFF
}

fun MemoryAllocator.debug_ptr_list(start: Ptr<Byte>): String {
    let buff = StringBuilder::new()
    let curr: Ptr<Byte> = start
    let iter = 0

    loop {
        if buff.is_not_empty() {
            buff[] = " -> "
        }
        buff[] = curr
        if curr.is_null() {
            break
        }
        if MemoryAllocator::is_bad_address(curr) {
            enable = false
            crash("Corrupted pointer while printing linked list: $curr")
            break
        }

        if iter > 100 {
            buff[] = "..."
            break
        }

        let int_ptr: IntPtr = curr.unsafe_cast_as_ref()
        curr = ptr_from_address<Byte>(int_ptr.value)
        iter += 1
    }

    ret buff.to_string()
}

fun MemoryAllocator.debug_detect_overlap(ptr: Ptr<Byte>, size: Int) {
    let block_ptr: Ptr<MemoryAllocatorBlock> = block_list

    while !block_ptr.is_null() {
        let block: MemoryAllocatorBlock = block_ptr.unsafe_as_ref()

        let start = block_ptr.address
        let end = start + MEMORY_ALLOCATOR_BLOCK_SIZE

        let start2 = ptr.address
        let end2 = start2 + size

        if start2 >= start && start2 < end {
            enable = false
            println("Overlap detected at ${ptr_of(block_ptr)}: $start <= start $start2 < $end, $size B")
            crash_with_info("Overlap detected")
        }

        if end2 > start && end2 < end {
            enable = false
            println("Overlap detected at ${ptr_of(block_ptr)}: $start < end $end2 < $end, $size B")
            crash_with_info("Overlap detected")
        }

        if start2 <= start && end2 > end {
            enable = false
            println("Overlap detected at ${ptr_of(block_ptr)}: $start <= start $start2 < end $end2 < $end, $size B")
            crash_with_info("Overlap detected")
        }

        if start2 >= start && end2 < end {
            enable = false
            println("Overlap detected at ${ptr_of(block_ptr)}: $start <= start $start2 < end $end2 < $end, $size B")
            crash_with_info("Overlap detected")
        }

        block_ptr = block.next
    }
}

// Print the internal state of the allocator
// Warning requires memory to print, cannot be used if the allocator is in a corrupted state
fun MemoryAllocator.debug() {
    enable = false
    println("---- BEGIN MemoryAllocator DUMP ----")
    println("MemoryAllocator: ${ptr_of(this)}")
    println("  user_alloc_total: $user_alloc_total")
    println("  user_free_total: $user_free_total")
    println("  total_available_entries: $total_available_entries")
    println("  block_list:")
    let block_ptr: Ptr<MemoryAllocatorBlock> = block_list
    let block_count = 0
    let entry_count = 0
    let used_entry_count = 0
    let free_entry_count = 0

    while !block_ptr.is_null() {
        let block: MemoryAllocatorBlock = block_ptr.unsafe_as_ref()

        println("    block: $block_ptr")
        println("      free_size: 0x${block.free_size.to_string_in_base(16)} (${block.free_size})")
        println("      next: ${block.next}")
        println("      entries:")
        repeat MEMORY_ALLOCATOR_ENTRIES_PER_BLOCK {
            let entry: MemoryAllocatorBlockEntry = block[it]
            let used = entry.flags & MASK_USED_ENTRY != 0
            let size = entry.flags & MASK_SIZE_ENTRY

            let new_ptr: Ptr<Byte> = if used {
                null_ptr<Byte>()
            } else {
                let int_ptr: IntPtr = entry.data.unsafe_cast_as_ref()
                ptr_from_address<Byte>(int_ptr.value)
            }

            println("        entry ${ptr_of(entry)} $it: used=$used, size=0x${size.to_string_in_base(16)} ($size), data=${entry.data}")
            if used {
                used_entry_count += 1
            } else {
                free_entry_count += 1
            }
            entry_count += 1
        }

        block_ptr = block.next
        block_count += 1
    }
    println("  block_count: $block_count")
    println("  entry_count: $entry_count ($used_entry_count used / $free_entry_count free)")
    println("---- END MemoryAllocator DUMP ----")
    enable = true
}

// MemoryAllocatorBlockEntry utilities

fun MemoryAllocatorBlockEntry::to_id(block: MemoryAllocatorBlock, index: Int): MemoryAllocatorBlockEntryId {
    ret (ptr_of(block).address.to_long() << 32L) | index.to_long()
}

fun MemoryAllocatorBlockEntry::get_entry_block_by_id(id: MemoryAllocatorBlockEntryId): MemoryAllocatorBlock {
    ret ptr_from_address<MemoryAllocatorBlock>((id >> 32L).to_int()).unsafe_as_ref()
}

fun MemoryAllocatorBlockEntry::get_entry_index_by_id(id: MemoryAllocatorBlockEntryId): Int {
    ret id.to_int() & MASK_SIZE_ENTRY
}

fun MemoryAllocatorBlockEntry::get_entry_by_id(id: MemoryAllocatorBlockEntryId): MemoryAllocatorBlockEntry {
    ret MemoryAllocatorBlockEntry::get_entry_block_by_id(id)[MemoryAllocatorBlockEntry::get_entry_index_by_id(id)]
}

fun MemoryAllocatorBlock.get(index: Int): MemoryAllocatorBlockEntry {
    let offset = MEMORY_ALLOCATOR_BLOCK_HEADER_SIZE + size_of<MemoryAllocatorBlockEntry>() * index
    ret ptr_of(this).offset_in_bytes(offset).unsafe_cast_as_ref()
}
