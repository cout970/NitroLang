
struct Lexer {
    // Original source code
    src: String
    file: String
    // Current byte position in the source code
    cur: Int
    // State stack, 0 is normal mode, 1 is string mode
    state: List<Int>
    // All the collected tokens
    tokens: List<Token>
    // Current token position in list of tokens
    tk: Int
    // All the ignored tokens, useful for documentation extractors, but unused by the compiler
    comments: List<Token>
}

// Map of identifier strings to keyword tokens
let keyword_map: Map<String, TokenKind> = #[
  "this": TokenKind::THIS,
  "This": TokenKind::THIS_TYPE,
  "fun": TokenKind::FUN,
  "let": TokenKind::LET,
  "mod": TokenKind::MODULE,
  "struct": TokenKind::STRUCT,
  "ret": TokenKind::RETURN,
  "return": TokenKind::RETURN,
  "size_of": TokenKind::SIZE_OF,
  "sizeOf": TokenKind::SIZE_OF,
  "sizeof": TokenKind::SIZE_OF,
  "option": TokenKind::OPTION,
  "internal": TokenKind::INTERNAL,
  "rec": TokenKind::REC,
  "tag": TokenKind::TAG,
  "defer": TokenKind::DEFER,
  "type_alias": TokenKind::TYPE_ALIAS,
  "typeAlias": TokenKind::TYPE_ALIAS,
  "typealias": TokenKind::TYPE_ALIAS,
  "enum": TokenKind::ENUM,
  "nothing": TokenKind::NOTHING,
  "when": TokenKind::WHEN,
  "match": TokenKind::MATCH,
  "alias": TokenKind::ALIAS,
  "if": TokenKind::IF,
  "else": TokenKind::ELSE,
  "for": TokenKind::FOR,
  "in": TokenKind::IN,
  "while": TokenKind::WHILE,
  "repeat": TokenKind::REPEAT,
  "loop": TokenKind::LOOP,
  "is": TokenKind::IS,
  "as": TokenKind::AS,
  "true": TokenKind::TRUE,
  "false": TokenKind::FALSE,
  "null": TokenKind::NULL,
  "include": TokenKind::INCLUDE,
  "break": TokenKind::BREAK,
  "continue": TokenKind::CONTINUE,
  "use": TokenKind::USE,
  "mut": TokenKind::MUT,
  "json!": TokenKind::JSON,
  "test!": TokenKind::TEST,
  "not": TokenKind::NOT,
  "or": TokenKind::OROR,
  "and": TokenKind::ANDAND,
  "xor": TokenKind::XORXOR,
  // Reserved (the annoying part of making a language by steps)
  "self": TokenKind::RESERVED,
  "Self": TokenKind::RESERVED,
  "function": TokenKind::RESERVED,
  "var": TokenKind::RESERVED,
  "val": TokenKind::RESERVED,
  "module": TokenKind::RESERVED,
  "class": TokenKind::RESERVED,
  "type": TokenKind::RESERVED,
  "recv": TokenKind::RESERVED,
  "receiver": TokenKind::RESERVED,
  "trait": TokenKind::RESERVED,
  "interface": TokenKind::RESERVED,
  "either": TokenKind::RESERVED,
  "ref_mut": TokenKind::RESERVED,
  "ref": TokenKind::RESERVED,
  "copy": TokenKind::RESERVED,
  "final": TokenKind::RESERVED,
  "local": TokenKind::RESERVED,
  "object": TokenKind::RESERVED,
  "package": TokenKind::RESERVED,
  "super": TokenKind::RESERVED,
  "throw": TokenKind::RESERVED,
  "try": TokenKind::RESERVED,
  "typeof": TokenKind::RESERVED,
  "finally": TokenKind::RESERVED,
  "then": TokenKind::RESERVED,
  "annotation": TokenKind::RESERVED,
  "inline": TokenKind::RESERVED,
  "expect": TokenKind::RESERVED,
  "infix": TokenKind::RESERVED,
  "private": TokenKind::RESERVED,
  "public": TokenKind::RESERVED,
  "protected": TokenKind::RESERVED,
  "suspend": TokenKind::RESERVED,
  "vararg": TokenKind::RESERVED,
]

// Creates a new lexer
fun Lexer::new(src: String, file: String): Lexer {
    ret Lexer @[
        src,
        file,
        cur: 0,
        state: [],
        tokens: [],
        tk: 0,
        comments: [],
    ]
}

// Read all available tokens
fun Lexer.read_all() {
    while next() {}
}

fun Lexer.next_token() {
    if tk < tokens.len - 4 {
        tk += 1
    }
}

// Returns the current token
fun Lexer.get_current_token(): Token {
    ret tokens[tk]!!
}

fun Lexer.get_current_token_kind(): TokenKind {
    ret tokens[tk]!!.kind
}

fun Lexer.get_next_non_nl_token_kind(): TokenKind {
    let i = tk

    while i < tokens.len {
        let token = tokens[i]!!
        if token.kind != TokenKind::NL {
            ret token.kind
        }
        i += 1
    }

    ret TokenKind::EOF
}

fun Lexer.get_current_token_text(): String {
    ret tokens[tk]!!.extra
}

fun Lexer.get_current_token_span(): Span {
    ret span_of(tokens[tk]!!)
}

// Returns the next token
fun Lexer.get_next_token_kind(): TokenKind {
    ret tokens[tk + 1]!!.kind
}

// Returns the token after the next token
fun Lexer.get_next_next_token_kind(): TokenKind {
    ret tokens[tk - 2]!!.kind
}

// Search for `target` before the next occurrence of `stop`
fun Lexer.find_token_kind_before(target: TokenKind, stop: TokenKind): Boolean {
    let curr = tk
    let token = tokens[curr]!!.kind

    while token != stop && token != TokenKind::EOF {
        if token == target {
            ret true
        }

        curr += 1
        token = tokens[curr]!!.kind
    }

    ret false
}

fun Lexer.find_token_kind_before2(target: TokenKind, stops: List<TokenKind>): Boolean {
    let curr = tk
    let token = tokens[curr]!!.kind

    while !stops.contains(token) && token != TokenKind::EOF {
        if token == target {
            ret true
        }

        curr += 1
        token = tokens[curr]!!.kind
    }

    ret false
}

type_alias SavedState = Int

// Save the current state of the lexer
fun Lexer.save_state(): SavedState {
    ret tk
}

// Restore the state of the lexer
fun Lexer.restore_state(saved_state: SavedState) {
    tk = saved_state
}

// Creates a new span from the given token
fun Lexer.span_of(token: Token): Span {
    let len = if token.kind.len > 0 { token.kind.len } else { token.extra.bytes_len }

    ret Span::new(token.offset, len, src, file)
}

// Returns the current byte in the source code
fun Lexer.get_current_byte(): Int {
    ret src.get_byte(cur).to_int()
}

// Returns the next byte in the source code
fun Lexer.get_next_byte(): Int {
    ret src.get_byte(cur + 1).to_int()
}

// Returns the byte next to the next byte
fun Lexer.get_next_next_byte(): Int {
    ret src.get_byte(cur + 2).to_int()
}

fun Lexer.add_token(kind: TokenKind) {
    tokens[] = Token::new(kind, cur)
}

fun Lexer.add_token(kind: TokenKind, offset: Int) {
    tokens[] = Token::new(kind, offset)
}

fun Lexer.add_token_with_extra(kind: TokenKind, offset: Int, extra: String) {
    tokens[] = Token::new_with_extra(kind, offset, extra)
}

// Tries to read the next token, returns true if there are more tokens to read
fun Lexer.next(): Boolean {
    if cur >= src.bytes_len {
        // We add extra so the parser can always look ahead 4 tokens
        add_token(TokenKind::EOF)
        add_token(TokenKind::EOF)
        add_token(TokenKind::EOF)
        add_token(TokenKind::EOF)
        ret false
    }

    let cur_state = if state.len > 0 { state.last()!! } else { 0 }

    if cur_state == 1 {
        read_string_blob()
        ret true
    }

    strip_whitespace()

    let c0: Int = current_byte

    when {
        // NL
        c0 == a"\n" -> {
            add_token(TokenKind::NL)
            cur += 1
        }
        // NL ;
        c0 == a";" -> {
            add_token(TokenKind::NL)
            cur += 1
        }
        // QUESTION_MARK    '?'
        c0 == a"?" -> {
            add_token(TokenKind::QUESTION_MARK)
            cur += 1
        }
        // UNDERSCORE       '_'
        c0 == a"_" -> {
            add_token(TokenKind::UNDERSCORE)
            cur += 1
        }
        // DOT              '.'
        c0 == a"." -> {
            let c1: Int = next_byte

            // Float .5
            if c1 >= a"0" && c1 <= a"9" {
                read_number()
                ret true
            }

            // Range ..< or ..=
            if c1 == a"." {
                let c2: Int = next_next_byte

                if c2 == a"<" {
                    add_token(TokenKind::RANGE_EXCLUSIVE)
                    cur += 3
                    ret true
                }

                if c2 == a"=" {
                    add_token(TokenKind::RANGE_INCLUSIVE)
                    cur += 3
                    ret true
                }
            }

            add_token(TokenKind::DOT)
            cur += 1
        }
        // LPAREN           '('
        c0 == a"(" -> {
            add_token(TokenKind::LPAREN)
            cur += 1
        }
        // RPAREN           ')'
        c0 == a")" -> {
            add_token(TokenKind::RPAREN)
            cur += 1
        }
        // LBRACE           '{'
        c0 == a"{" -> {
            add_token(TokenKind::LBRACE)
            cur += 1
        }
        // LBRACE           '{'
        c0 == a"}" -> {

            // If we are inside a string interpolation, we need to close it
            if state.len > 0 {
                state.remove_last()!!

                cur_state = if state.len > 0 { state[state.len - 1]!! } else { 0 }

                if cur_state != 0 {
                    add_token(TokenKind::STRING_INTERP_END)
                    cur += 1
                    ret true
                }
            }

            add_token(TokenKind::RBRACE)
            cur += 1
        }
        // LBRACKET         '['
        c0 == a"[" -> {
            add_token(TokenKind::LBRACKET)
            cur += 1
        }
        // RBRACKET         ']'
        c0 == a"]" -> {
            add_token(TokenKind::RBRACKET)
            cur += 1
        }
        // COMMA            ','
        c0 == a"," -> {
            add_token(TokenKind::COMMA)
            cur += 1
        }
        // COLON            ':'
        c0 == a":" -> {
            let c1: Int = next_byte

            // ::
            if c1 == a":" {
                add_token(TokenKind::DOUBLE_COLON)
                cur += 2
                ret true
            }

            add_token(TokenKind::COLON)
            cur += 1
        }
        // ADD              '+'
        c0 == a"+" -> {
            let c1: Int = next_byte

            // +5
            if c1 >= a"0" && c1 <= a"9" {
                read_number()
                ret true
            }

            // +=
            if c1 == a"=" {
                add_token(TokenKind::ADD_ASSIGN)
                cur += 2
                ret true
            }

            add_token(TokenKind::ADD)
            cur += 1
        }
        // SUB              '-'
        c0 == a"-" -> {
            let c1: Int = next_byte

            // -5
            if c1 >= a"0" && c1 <= a"9" {
                read_number()
                ret true
            }

            // ->
            if c1 == a">" {
                add_token(TokenKind::ARROW)
                cur += 2
                ret true
            }

            // -=
            if c1 == a"=" {
                add_token(TokenKind::SUB_ASSIGN)
                cur += 2
                ret true
            }

            add_token(TokenKind::SUB)
            cur += 1
        }
        // MUL              '*'
        c0 == a"*" -> {
            let c1: Int = next_byte

            // *=
            if c1 == a"=" {
                add_token(TokenKind::MUL_ASSIGN)
                cur += 2
                ret true
            }

            add_token(TokenKind::MUL)
            cur += 1
        }
        // DIV              '/'
        c0 == a"/" -> {
            let c1: Int = next_byte

            // /=
            if c1 == a"=" {
                add_token(TokenKind::DIV_ASSIGN)
                cur += 2
                ret true
            }

            // //
            if c1 == a"/" {
                read_line_comment()
                ret true
            }

            // /*
            if c1 == a"*" {
                read_block_comment()
                ret true
            }

            add_token(TokenKind::DIV)
            cur += 1
        }
        // XOR              '^'
        c0 == a"^" -> {
            let c1: Int = next_byte

            // ^^
            if c1 == a"^" {
                add_token(TokenKind::XORXOR)
                cur += 2
                ret true
            }

            add_token(TokenKind::XOR)
            cur += 1
        }
        // DOLAR            '$'
        c0 == a"\$" -> {
            add_token(TokenKind::DOLAR)
            cur += 1
        }
        // MOD              '%'
        c0 == a"%" -> {
            let c1: Int = next_byte

            // %=
            if c1 == a"=" {
                add_token(TokenKind::MOD_ASSIGN)
                cur += 2
                ret true
            }

            // %[
            if c1 == a"[" {
                add_token(TokenKind::SET_START)
                cur += 2
                ret true
            }


            add_token(TokenKind::MOD)
            cur += 1
        }
        // AT               '@'
        c0 == a"@" -> {
            let c1: Int = next_byte

            // @[
            if c1 == a"[" {
                add_token(TokenKind::STRUCT_START)
                cur += 2
                ret true
            }

            // @{
            if c1 == a"{" {
                add_token(TokenKind::LAMBDA_START)
                cur += 2
                ret true
            }

            add_token(TokenKind::AT)
            cur += 1
        }
        // HASH             '#'
        c0 == a"#" -> {
            let c1: Int = next_byte

            // #[
            if c1 == a"[" {
                add_token(TokenKind::MAP_START)
                cur += 2
                ret true
            }

            add_token(TokenKind::HASH)
            cur += 1
        }
        // OR               '|'
        c0 == a"|" -> {
            let c1: Int = next_byte

            // ||
            if c1 == a"|" {
                add_token(TokenKind::OROR)
                cur += 2
                ret true
            }

            add_token(TokenKind::OR)
            cur += 1
        }
        // AND              '&'
        c0 == a"&" -> {
            let c1: Int = next_byte

            // &&
            if c1 == a"&" {
                add_token(TokenKind::ANDAND)
                cur += 2
                ret true
            }

            add_token(TokenKind::AND)
            cur += 1
        }
        // NOT               '!' ;
        c0 == a"!" -> {
            let c1: Int = next_byte

            // !=
            if c1 == a"=" {
                add_token(TokenKind::NEQ)
                cur += 2
                ret true
            }

            // !!
            if c1 == a"!" {
                add_token(TokenKind::BANGBANG)
                cur += 2
                ret true
            }

            add_token(TokenKind::NOT)
            cur += 1
        }
        // ASSIGN            '=' ;
        c0 == a"=" -> {
            let c1: Int = next_byte

            // ==
            if c1 == a"=" {
                add_token(TokenKind::EQ)
                cur += 2
                ret true
            }

            add_token(TokenKind::ASSIGN)
            cur += 1
        }
        // LTH               '<' ;
        c0 == a"<" -> {
            let c1: Int = next_byte

            // <=>
            if c1 == a"=" {
                let c2: Int = next_next_byte

                if c2 == a">" {
                    add_token(TokenKind::COMPARE)
                    cur += 3
                    ret true
                }
            }

            // <=
            if c1 == a"=" {
                add_token(TokenKind::LEQ)
                cur += 2
                ret true
            }

            add_token(TokenKind::LTH)
            cur += 1
        }
        // GTH               '>' ;
        c0 == a">" -> {
            let c1: Int = next_byte

            // >=
            if c1 == a"=" {
                add_token(TokenKind::GEQ)
                cur += 2
                ret true
            }

            add_token(TokenKind::GTH)
            cur += 1
        }
        // string
        c0 == a"\"" -> {
            read_string()
            ret true
        }
        // number
        c0 >= a"0" && c0 <= a"9" -> {
            read_number()
            ret true
        }
        // identifier
        c0 >= a"a" && c0 <= a"z" || c0 >= a"A" && c0 <= a"Z" -> {
            // r#"
            if c0 == a"r" && next_byte == a"#" && next_next_byte == a"\"" {
                read_string2()
                ret true
            }

            read_identifier()
            ret true
        }

        c0 == a"`" -> {
            let c1: Int = next_byte
            let c2: Int = next_next_byte

            if c1 == a"`" && c2 == a"`" {
                read_foreign_block()
                ret true
            }

            cur += 1
        }

        else -> {
            add_token(TokenKind::ERROR_CHARACTER)
            cur += 1
        }
    }

    ret true
}

// Skip whitespaces
fun Lexer.strip_whitespace() {
    while cur < src.bytes_len {
        let c: Int = current_byte

        // c is ' ', '\t', '\r'
        if c == 32 || c == 9 || c == 13 {
            cur += 1
        } else {
            break
        }
    }
}

// Read a single line comment
//
// // This is a line comment
//
fun Lexer.read_line_comment() {
    let start = cur

    while cur < src.bytes_len {
        let c: Int = current_byte

        if c == a"\n" {
            break
        }

        cur += 1
    }

    let extra = src.substring(start, cur)
    comments[] = Token::new_with_extra(TokenKind::LINE_COMMENT, start, extra)
}

// Read a block comment
//
// /**
//  * This is a documentation block comment
//  */
// or
// /*
//  * This is a regular block comment
//  */
//
fun Lexer.read_block_comment() {
    // skip "/*"
    cur += 2
    let start = cur
    let max = src.bytes_len - 1

    while cur < max {
        let c: Int = current_byte

        if c == a"*" {
            let c1: Int = next_byte

            if c1 == a"/" {
                break
            }
        }

        cur += 1
    }

    let extra = src.substring(start, cur)
    cur += 2

    // Detect /** vs /* at the beginning of a comment
    let kind = if extra.bytes_len > 1 && extra.get_byte(0).to_int() == a"*" {
        TokenKind::DOC_COMMENT
    } else {
        TokenKind::BLOCK_COMMENT
    }

    comments[] = Token::new_with_extra(kind, start, extra)
}

// Read a foreign block
//
// This special kind of comment block allows to embed foreign code: markdown, javascript, etc.
//
// ```md
// # Inline doc
// - Step 1
// - Step 2
// - Step 3
// ```
fun Lexer.read_foreign_block() {
    // skip ```
    cur += 3

    let start = cur
    let max = src.bytes_len

    while cur < max {
        let c: Int = current_byte

        if current_byte == a"`" && next_byte == a"`" && next_next_byte == a"`"{
            break
        }

        cur += 1
    }

    let extra = src.substring(start, cur)

    // skip ```
    cur += 3

    add_token_with_extra(TokenKind::FOREIGN_BLOCK, start, extra)
}

// Read a string
//
// There are two kinds of strings:
// - plain strings: "foo"
// - interpolated strings: "foo $bar ${1 + 2} baz"
//
// Plain strings are simple strings without any interpolation, may contain escape characters.
// Interpolated strings can contain expressions inside `${}` or `$name` and require special handling.
//
fun Lexer.read_string() {
    // skip '"'
    cur += 1
    let start = cur
    let is_plain = true

    while cur < src.bytes_len {
        let c: Int = current_byte

        if c == a"\"" {
            break
        }

        if c == a"\$" || c == a"\\" {
            is_plain = false
            break
        }

        cur += 1
    }

    if is_plain {

    let extra = src.substring(start, cur)
        // skip '"'
        cur += 1
        add_token_with_extra(TokenKind::PLAIN_STRING, start, extra)
        ret
    }

    // Try again but with interpolation
    cur = start
    add_token(TokenKind::STRING_START)
    state[] = 1
}

// Read an ascii string
// a"S"
fun Lexer.read_ascii_string() {
    let start = cur

    if current_byte != a"a" {
        add_token(TokenKind::ERROR_CHARACTER)
        cur += 1
        ret
    }
    cur += 1

    if current_byte != a"\"" {
        add_token(TokenKind::ERROR_CHARACTER)
        cur += 1
        ret
    }
    cur += 1

    let c0: Int = current_byte
    let byte = c0

    if c0 == a"\\" {
        let seq = read_string_escape_sequence()
        byte = seq.get_byte(0).to_int()
    } else {
        cur += 1
    }

    if current_byte != a"\"" {
        add_token(TokenKind::ERROR_CHARACTER)
        cur += 1
        ret
    }

    // skip "
    cur += 1
    let extra = byte.to_string_in_base(10)

    add_token_with_extra(TokenKind::ASCII_STRING, start, extra)
}

// Read a string contents
//
// The contents of a string can be:
// - a blob of text
// - an escape character
// - a variable name
// - an interpolation expression
// The string ends with a double quote.
//
// This function generates a single token for each of the above cases.
//
fun Lexer.read_string_blob() {
    let start = cur
    let c0 = current_byte

    if c0 == a"\"" {
        cur += 1
        add_token(TokenKind::STRING_END, start)
        state.remove_last()!!
        ret
    }

    if c0 == a"\\" {
        let extra = read_string_escape_sequence()

        add_token_with_extra(TokenKind::STRING_BLOB, start, extra)
        ret
    }

    if c0 == a"\$" {
        let c1 = next_byte

        if c1 == a"{" {
            cur += 2
            add_token(TokenKind::STRING_INTERP_START, start)
            state[] = 0
            ret
        }

        if c1 >= a"a" && c1 <= a"z" {
            cur += 1
            let start2 = cur

            while cur < src.bytes_len {
                let c: Int = current_byte

                if c >= a"a" && c <= a"z" || c >= a"A" && c <= a"Z" || c >= a"0" && c <= a"9" || c == a"_" {
                    cur += 1
                } else {
                    break
                }
            }

            let extra = src.substring(start2, cur)
            add_token_with_extra(TokenKind::STRING_VAR, start2, extra)
            ret
        }
    }

    while cur < src.bytes_len {
        let c: Int = current_byte

        if c == a"\$" || c == a"\\" || c == a"\"" {
            break
        }

        cur += 1
    }
    let extra = src.substring(start, cur)

    add_token_with_extra(TokenKind::STRING_BLOB, start, extra)
}

// Reads an string escape sequence, i.e \n, \t, \xFF, \uFFFF, \(, \$, etc.
fun Lexer.read_string_escape_sequence(): String {
    let c0 = current_byte

    if c0 != a"\\" {
        add_token(TokenKind::ERROR_CHARACTER)
        cur += 1
        ret ""
    }

    // Ignore '\'
    cur += 1
    let c1 = current_byte

    // Hex \xFF
    if c1 == a"x" {
        // Ignore 'x'
        cur += 1
        let c2 = current_byte
        cur += 1
        let c3 = current_byte
        cur += 1

        let hex = c2.to_char() + c3.to_char()
        ret hex.to_int_in_base(16)!!.to_char().to_string()
    }

    // Unicode \uFFFF
    if c1 == a"u" {
        // Ignore 'u'
        cur += 1

        let c2 = current_byte
        cur += 1
        let c3 = current_byte
        cur += 1
        let c4 = current_byte
        cur += 1
        let c5 = current_byte
        cur += 1

        let hex = c2.to_char() + c3.to_char() + c4.to_char() + c5.to_char()

        let value = hex.to_int_in_base(16)!!
        ret value.to_char().to_string()
    }

    // C escape codes \n
    let extra = ""
    when c1 {
        a"0" -> { extra = "\0" }
        a"n" -> { extra = "\n" }
        a"r" -> { extra = "\r" }
        a"t" -> { extra = "\t" }
        a"a" -> { extra = "\u0007" }
        a"b" -> { extra = "\u0008" }
        a"e" -> { extra = "\u001b" }
        a"f" -> { extra = "\u000c" }
        a"v" -> { extra = "\u000b" }
        // Already handled by the last case, that works on any special character
//            a"\"" -> { extra = "\"" }
//            a"\$" -> { extra = "\$" }
//            a"\'" -> { extra = "\'" }
//            a"\\" -> { extra = "\\" }
        else -> { extra = c1.to_char().to_string() }
    }
    cur += 1

    ret extra
}

// Read a raw string without escape characters and without interpolation
//
// r#"
// This is a raw string
// "#
//
fun Lexer.read_string2() {
    // skip r#"
    cur += 3
    add_token(TokenKind::STRING2_START)

    let start = cur
    let max = src.bytes_len - 1

    while cur < max {
        if current_byte == a"\"" && next_byte == a"#" {
            cur += 2
            break
        }

        cur += 1
    }

    let extra = src.substring(start, cur - 2)
    if extra.is_not_empty() {
        add_token_with_extra(TokenKind::STRING2_BLOB, start, extra)
    }
    add_token(TokenKind::STRING2_END)
}

// Read an identifier
//
// There are two kinds of identifiers:
// - lower case identifiers: `foo`
// - upper case identifiers: `Foo`
//
// Identifiers can contain letters, digits and underscores, but cannot start with a digit or underscore.
// Also identifiers cannot end with an underscore.
//
fun Lexer.read_identifier() {
    let start = cur

    while cur < src.bytes_len {
        let c: Int = current_byte

        if c >= a"a" && c <= a"z" || c >= a"A" && c <= a"Z" || c >= a"0" && c <= a"9" || c == a"_" {
            cur += 1
        } else {
            break
        }
    }

    let extra = src.substring(start, cur)

    // Identifiers cannot end in '_'
    if extra.get_byte(extra.bytes_len - 1).to_int() == a"_" {
        cur -= 1
        extra = src.substring(start, cur)
    }

    // Keyword
    let keyword = keyword_map[extra]

    if keyword.is_some() {
        add_token(keyword!!, start)
        ret
    }

    if current_byte == a"!" {
        // test!
        if extra == "test" {
            add_token(TokenKind::TEST, start)
            cur += 1
            ret
        }

        // json!
        if extra == "json" {
            add_token(TokenKind::JSON, start)
            cur += 1
            ret
        }
    }

    // Identifier
    let first = extra.get_byte(0).to_int()
    let lower = first >= a"a" && first <= a"z"

    // Ascii string
    if first == a"a" && extra.bytes_len == 1 && current_byte == a"\"" {
        cur -= 1
        read_ascii_string()
        ret
    }

    add_token_with_extra(TokenKind::IDENTIFIER, start, extra)
}

// Read a number
//
// A number can be an integer, long or float.
//
// Integers can be written in decimal, hexadecimal, octal or binary, using the following prefixes:
// - 0x for hexadecimal
// - 0o for octal
// - 0b for binary
// - no prefix for decimal
//
// Floats look like this:
// - 1.0
// - .025
// - 1.0f
// - +1f
// - -1f
// - -1.23e10
// - -1.23e+10
// - 1.23e-10F
// - +1.23e-10d
//
fun Lexer.read_number() {
    let start = cur

    // hex number 0xFF
    if current_byte == a"0" && next_byte == a"x" {
        cur += 2

        while cur < src.bytes_len {
            let c: Int = current_byte

            if c >= a"0" && c <= a"9" || c >= a"a" && c <= a"f" || c >= a"A" && c <= a"F" || c == a"_" {
                cur += 1
            } else {
                break
            }
        }

        let extra = src.substring(start, cur)
        add_token_with_extra(TokenKind::INT_NUMBER, start, extra)
        ret
    }

    // octal number 0o777
    if current_byte == a"0" && next_byte == a"o" {
        cur += 2

        while cur < src.bytes_len {
            let c: Int = current_byte

            if c >= a"0" && c <= a"7" || c == a"_" {
                cur += 1
            } else {
                break
            }
        }

        let extra = src.substring(start, cur)
        add_token_with_extra(TokenKind::INT_NUMBER, start, extra)
        ret
    }

    // binary number 0b111
    if current_byte == a"0" && next_byte == a"b" {
        cur += 2

        while cur < src.bytes_len {
            let c: Int = current_byte

            if c >= a"0" && c <= a"1" || c == a"_" {
                cur += 1
            } else {
                break
            }
        }

        let extra = src.substring(start, cur)
        add_token_with_extra(TokenKind::INT_NUMBER, start, extra)
        ret
    }

    if current_byte == a"+" {
       cur += 1
    }

    if current_byte == a"-" {
        cur += 1
    }

    let is_float = false
    let dot_read = false

    // '.' DIGITS | DIGITS ('.' DIGITS)?
    // Exception 1..=2, starts with 1., but is not a float
    if current_byte == a"." && next_byte >= a"0" && next_byte <= a"9" {
        is_float = true
        dot_read = true
        cur += 1
    }

    read_digits()

    if !dot_read && current_byte == a"." && next_byte >= a"0" && next_byte <= a"9" {
        is_float = true
        cur += 1

        read_digits()
    }

    // [eE][+-]?DIGITS
    if current_byte == a"e" || current_byte == a"E" {
        is_float = true
        cur += 1

        if current_byte == a"+" || current_byte == a"-" {
            cur += 1
        }

        read_digits()
    }

    // [fF]
    if current_byte == a"f" || current_byte == a"F" {
        is_float = true
        cur += 1
    }

    // [lL]
    if !is_float && (current_byte == a"l" || current_byte == a"L") {
        cur += 1

        let extra = src.substring(start, cur)
        add_token_with_extra(TokenKind::LONG_NUMBER, start, extra)
        ret
    }

    let extra = src.substring(start, cur)
    let kind = if is_float { TokenKind::FLOAT_NUMBER } else { TokenKind::INT_NUMBER }

    add_token_with_extra(kind, start, extra)
}

// Read a sequence of consecutive digits
// Returns true if at least one digit was read
fun Lexer.read_digits(): Boolean {
    let start = cur
    let prev_was_digit = false

    while cur < src.bytes_len {
        let c: Int = current_byte

        // ignore '_' between digits
        if c == a"_" && prev_was_digit && next_byte >= a"0" && next_byte <= a"9" {
            cur += 1
            prev_was_digit = false
            continue
        }

        if c < a"0" || c > a"9" {
            // Non digit
            break
        }

        prev_was_digit = true
        cur += 1
    }

    ret start != cur
}

fun Lexer.print_debug_info() {
    println("Tokens around print_debug_info():")
    let start = max(tk - 5, 0)
    let end = min(tk + 5, tokens.len - 1)

    while start <= end {
        let token = tokens[start]!!
        let span = span_of(token)
        println("[${start - tk}] $token $span")
        start += 1
    }
}

fun Lexer.print_char_debug_info() {
    println("Chars around print_char_debug_info():")
    let start = max(cur - 5, 0)
    let end = min(cur + 5, src.bytes_len - 1)

    while start <= end {
        let byte = src.get_byte(start).to_int()
        println("[${start - cur}] $byte => ${byte.to_char()} ")
        start += 1
    }
}